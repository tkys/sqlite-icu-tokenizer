#!/usr/bin/env python3
"""
ICU Query Expansion Demo
Demonstrates how to tokenize long text queries using ICU and expand them to OR queries for improved hit rates.
"""

import sqlite3
import re
from typing import List, Tuple, Dict

class ICUQueryExpander:
    """
    Query expansion using ICU tokenization for improved search hit rates.
    """
    
    def __init__(self, db_path: str = ":memory:"):
        """Initialize with SQLite database and ICU extension."""
        self.conn = sqlite3.connect(db_path)
        self.conn.execute("PRAGMA foreign_keys = ON")
        
        # Load ICU extension
        try:
            self.conn.enable_load_extension(True)
            self.conn.load_extension("./fts5icu.so")
            print("âœ… ICU extension loaded successfully")
        except Exception as e:
            print(f"âŒ Failed to load ICU extension: {e}")
            print("Note: Make sure fts5icu.so is built and in the current directory")
            print("Continuing with simplified tokenization demo...")
            # Continue without ICU extension for demo purposes
            self.use_icu = False
        else:
            self.use_icu = True
    
    def setup_demo_data(self):
        """Set up demo tables and data."""
        # Create FTS5 table with ICU tokenizer if available, otherwise use unicode61
        tokenizer = 'icu' if self.use_icu else 'unicode61'
        self.conn.execute(f"""
            CREATE VIRTUAL TABLE IF NOT EXISTS docs USING fts5(
                id, title, content, tokenize='{tokenizer}'
            )
        """)
        print(f"ğŸ“Š Using tokenizer: {tokenizer}")
        
        # Insert demo data
        demo_docs = [
            (1, 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆå…¥é–€', 'ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ­£è¦åŒ–æ‰‹æ³•ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¾ã™ã€‚SQLiteã®å®Ÿè·µçš„ãªæ´»ç”¨æ–¹æ³•ã‚‚å«ã¿ã¾ã™ã€‚'),
            (2, 'æ©Ÿæ¢°å­¦ç¿’ã¨Python', 'Pythonã‚’ä½¿ã£ãŸæ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤ã‹ã‚‰å¿œç”¨ã¾ã§ã€‚scikit-learnã€pandasã€numpyãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®åŠ¹æœçš„ãªä½¿ç”¨æ–¹æ³•ã€‚'),
            (3, 'ã‚¦ã‚§ãƒ–é–‹ç™ºãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯', 'HTMLã¨CSSã‚’ä½¿ã£ãŸãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰é–‹ç™ºã€‚JavaScriptã¨Reactã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚‚è©³ã—ãç´¹ä»‹ã€‚'),
            (4, 'ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ', 'ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã®åŸå‰‡ã¨å®Ÿè·µã€‚ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é¸æŠã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã€‚'),
            (5, 'è‡ªç„¶è¨€èªå‡¦ç†æŠ€è¡“', 'ICUãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æ´»ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆè§£æã¨æ—¥æœ¬èªå‡¦ç†ã€‚ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€å½¢æ…‹ç´ è§£æã®å®Ÿè£…æ‰‹æ³•ã€‚'),
            (6, 'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹å®Ÿè·µ', 'ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²ã‚æ–¹ã€‚çµ±è¨ˆå­¦ã€æ©Ÿæ¢°å­¦ç¿’ã€å¯è¦–åŒ–æŠ€è¡“ã®ç·åˆçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚'),
            (7, 'ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¤ãƒ³ãƒ•ãƒ©æ§‹ç¯‰', 'AWSã€Azureã€GCPã‚’ä½¿ã£ãŸã‚¯ãƒ©ã‚¦ãƒ‰ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã€‚ã‚³ãƒ³ãƒ†ãƒŠæŠ€è¡“ã€Kubernetesã€DevOpsã®å°å…¥ã€‚'),
            (8, 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…', 'æƒ…å ±ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®åŸºæœ¬åŸå‰‡ã¨å®Ÿè£…ã€‚æš—å·åŒ–ã€èªè¨¼ã€ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ã€è„…å¨å¯¾ç­–ã«ã¤ã„ã¦ã€‚'),
        ]
        
        self.conn.executemany("INSERT OR REPLACE INTO docs VALUES (?, ?, ?)", demo_docs)
        print(f"âœ… Demo data inserted: {len(demo_docs)} documents")
    
    def tokenize_query_with_icu(self, query_text: str) -> List[str]:
        """
        Use ICU tokenization to break down query text into tokens.
        This simulates how ICU would tokenize the input.
        """
        # Create a temporary table for tokenization analysis
        self.conn.execute("""
            CREATE TEMP TABLE IF NOT EXISTS tokenize_temp (
                text TEXT
            ) 
        """)
        
        # Insert query text
        self.conn.execute("DELETE FROM tokenize_temp")
        self.conn.execute("INSERT INTO tokenize_temp VALUES (?)", (query_text,))
        
        # For demonstration, we'll extract meaningful terms
        # In a real implementation, you might use ICU's tokenization directly
        japanese_pattern = r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+'
        english_pattern = r'[A-Za-z]+(?:[A-Za-z0-9]*[A-Za-z][A-Za-z0-9]*)*'
        
        tokens = []
        
        # Extract Japanese terms (2+ characters)
        japanese_matches = re.findall(japanese_pattern, query_text)
        for match in japanese_matches:
            if len(match) >= 2:
                tokens.append(match)
        
        # Extract English terms (2+ characters)
        english_matches = re.findall(english_pattern, query_text)
        for match in english_matches:
            if len(match) >= 2:
                tokens.append(match)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_tokens = []
        for token in tokens:
            if token not in seen:
                seen.add(token)
                unique_tokens.append(token)
        
        return unique_tokens
    
    def expand_query(self, original_query: str, strategy: str = "comprehensive") -> str:
        """
        Expand a long query into an OR-connected FTS5 query.
        
        Args:
            original_query: The original long text query
            strategy: "basic", "comprehensive", or "progressive"
        
        Returns:
            Expanded FTS5-compatible query string
        """
        tokens = self.tokenize_query_with_icu(original_query)
        
        if not tokens:
            return original_query
        
        if strategy == "basic":
            # Simple OR connection of all tokens
            return " OR ".join(f'"{token}"' for token in tokens)
        
        elif strategy == "comprehensive":
            # Include both exact tokens and potential variations
            expanded_terms = []
            for token in tokens:
                expanded_terms.append(token)
                
                # Add related terms based on common patterns
                if 'ãƒ‡ãƒ¼ã‚¿' in token:
                    expanded_terms.extend(['æƒ…å ±', 'çµ±è¨ˆ'])
                if 'æ©Ÿæ¢°å­¦ç¿’' in token:
                    expanded_terms.extend(['AI', 'äººå·¥çŸ¥èƒ½', 'Python'])
                if 'ã‚·ã‚¹ãƒ†ãƒ ' in token:
                    expanded_terms.extend(['ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£', 'è¨­è¨ˆ', 'é–‹ç™º'])
            
            # Remove duplicates
            unique_terms = list(dict.fromkeys(expanded_terms))
            return " OR ".join(unique_terms)
        
        elif strategy == "progressive":
            # Create a weighted query with different priorities
            if len(tokens) <= 2:
                return " AND ".join(tokens)
            elif len(tokens) <= 4:
                return " OR ".join(tokens)
            else:
                # Use most important terms with AND, others with OR
                core_terms = tokens[:3]
                additional_terms = tokens[3:]
                core_query = " AND ".join(core_terms)
                if additional_terms:
                    additional_query = " OR ".join(additional_terms)
                    return f"({core_query}) OR ({additional_query})"
                else:
                    return core_query
        
        else:
            return " OR ".join(tokens)
    
    def search_with_expansion(self, query: str, strategy: str = "comprehensive") -> List[Tuple]:
        """
        Perform search with query expansion and return results.
        """
        print(f"\nğŸ” Original query: '{query}'")
        
        # Tokenize and expand
        tokens = self.tokenize_query_with_icu(query)
        print(f"ğŸ“ Extracted tokens: {tokens}")
        
        expanded_query = self.expand_query(query, strategy)
        print(f"ğŸš€ Expanded query: '{expanded_query}'")
        
        # Perform search
        try:
            cursor = self.conn.execute("""
                SELECT id, title, 
                       highlight(docs, 1, '<mark>', '</mark>') as highlighted_title,
                       snippet(docs, 2, '[', ']', '...', 15) as content_snippet
                FROM docs 
                WHERE docs MATCH ?
                ORDER BY bm25(docs)
                LIMIT 10
            """, (expanded_query,))
            
            results = cursor.fetchall()
            print(f"âœ… Found {len(results)} results")
            
            return results
            
        except Exception as e:
            print(f"âŒ Search error: {e}")
            return []
    
    def compare_strategies(self, query: str):
        """Compare different expansion strategies."""
        print(f"\n{'='*60}")
        print(f"COMPARING EXPANSION STRATEGIES FOR: '{query}'")
        print(f"{'='*60}")
        
        strategies = ["basic", "comprehensive", "progressive"]
        
        for strategy in strategies:
            print(f"\n--- Strategy: {strategy.upper()} ---")
            results = self.search_with_expansion(query, strategy)
            
            for i, (doc_id, title, highlighted, snippet) in enumerate(results, 1):
                print(f"{i}. [{doc_id}] {highlighted}")
                print(f"   {snippet}")
    
    def demo_scenarios(self):
        """Run demonstration scenarios."""
        scenarios = [
            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã¨æ©Ÿæ¢°å­¦ç¿’ã®çµ±åˆã‚·ã‚¹ãƒ†ãƒ é–‹ç™º",
            "Pythonã‚’ä½¿ã£ãŸè‡ªç„¶è¨€èªå‡¦ç†ã¨ã‚¦ã‚§ãƒ–ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ§‹ç¯‰",
            "ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹åˆ†æåŸºç›¤è¨­è¨ˆ",
            "ã‚»ã‚­ãƒ¥ã‚¢ãªãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å®Ÿè£…æ–¹æ³•",
        ]
        
        for scenario in scenarios:
            self.compare_strategies(scenario)
    
    def performance_analysis(self):
        """Analyze performance of different query lengths."""
        print(f"\n{'='*60}")
        print("PERFORMANCE ANALYSIS")
        print(f"{'='*60}")
        
        test_cases = [
            ("çŸ­ã„", "æ©Ÿæ¢°å­¦ç¿’"),
            ("ä¸­ç¨‹åº¦", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã®æ–¹æ³•"),
            ("é•·ã„", "Pythonã‚’ä½¿ã£ãŸãƒ‡ãƒ¼ã‚¿åˆ†æãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ"),
            ("éå¸¸ã«é•·ã„", "ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªã‚¯ãƒ©ã‚¦ãƒ‰ãƒ™ãƒ¼ã‚¹æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆã¨å®Ÿè£…ã«ãŠã‘ã‚‹æœ€é©åŒ–æ‰‹æ³•"),
        ]
        
        for length_desc, query in test_cases:
            print(f"\n--- {length_desc}ã‚¯ã‚¨ãƒª: '{query}' ---")
            
            # Original single-term search
            try:
                cursor = self.conn.execute(
                    "SELECT COUNT(*) FROM docs WHERE docs MATCH ?", 
                    (query,)
                )
                original_count = cursor.fetchone()[0]
            except:
                original_count = 0
            
            # Expanded search
            expanded_query = self.expand_query(query, "comprehensive")
            try:
                cursor = self.conn.execute(
                    "SELECT COUNT(*) FROM docs WHERE docs MATCH ?", 
                    (expanded_query,)
                )
                expanded_count = cursor.fetchone()[0]
            except Exception as e:
                print(f"Error with expanded query: {e}")
                expanded_count = 0
            
            improvement = expanded_count - original_count
            print(f"  Original: {original_count} hits")
            print(f"  Expanded: {expanded_count} hits")
            print(f"  Improvement: +{improvement} hits ({improvement/max(1,original_count)*100:.1f}% increase)")
    
    def close(self):
        """Close database connection."""
        self.conn.close()


def main():
    """Main demonstration function."""
    print("ğŸš€ ICU Query Expansion Demo")
    print("=" * 40)
    
    # Initialize expander
    expander = ICUQueryExpander()
    
    try:
        # Set up demo data
        expander.setup_demo_data()
        
        # Run demonstration scenarios
        expander.demo_scenarios()
        
        # Performance analysis
        expander.performance_analysis()
        
        print(f"\n{'='*60}")
        print("âœ… Demo completed successfully!")
        print("Key findings:")
        print("- Query expansion significantly improves hit rates")
        print("- ICU tokenization helps extract meaningful terms")
        print("- Different strategies serve different use cases")
        print("- Progressive expansion balances precision and recall")
        
    finally:
        expander.close()


if __name__ == "__main__":
    main()